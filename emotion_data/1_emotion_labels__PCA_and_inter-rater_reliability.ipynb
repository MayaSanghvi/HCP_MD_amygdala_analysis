{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa7200a-cc60-4038-a59b-9db2521631ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check assumptions before computing PCA\n",
    "\n",
    "## start by parsing data (example here on portrayed emotion data)\n",
    "## you will need: portrayed_emotions_git.csv and \n",
    "## viewer_emotions_git\n",
    "\n",
    "import re, numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# A. read raw file (unchanged)\n",
    "df = pd.read_csv(\"portrayed_emotions_git.csv\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# B. convert cells that contain several numbers to their mean\n",
    "#    – looks for commas, semicolons, pipes, or whitespace as separators\n",
    "#    – leaves single numbers untouched\n",
    "#    – returns NaN on parsing problems so they can be handled later\n",
    "# ------------------------------------------------------------------\n",
    "_multi_val_pattern = re.compile(r'[,\\|;]|\\s+')\n",
    "\n",
    "def _average_if_multi(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    if isinstance(x, str):\n",
    "        parts = [p for p in _multi_val_pattern.split(x) if p]\n",
    "        if len(parts) > 1:                       # ⇠ multi‑value cell\n",
    "            try:\n",
    "                nums = list(map(float, parts))\n",
    "                return float(np.mean(nums))\n",
    "            except ValueError:                   # contains non‑numeric text\n",
    "                return np.nan\n",
    "        else:                                    # single value as text\n",
    "            try:\n",
    "                return float(parts[0])\n",
    "            except ValueError:\n",
    "                return np.nan\n",
    "    return x                                     # already numeric\n",
    "\n",
    "df = df.applymap(_average_if_multi)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# C. keep only numeric variables to analyze\n",
    "df = df.select_dtypes(include=\"number\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# D. handle missing values\n",
    "df = df.dropna()           # or any imputation strategy you prefer\n",
    "\n",
    "## now: check assumptions -- with KMO and Bartlett's tests\n",
    "\n",
    "##KMO test\n",
    "from factor_analyzer import calculate_kmo\n",
    "\n",
    "kmo_per_item, kmo_overall = calculate_kmo(df.values)\n",
    "\n",
    "print(f\"Overall KMO = {kmo_overall:0.3f}\")          # e.g. 0.77\n",
    "\n",
    "# to see which items are dragging things down:\n",
    "kmo_table = pd.Series(kmo_per_item, index=df.columns).sort_values()\n",
    "print(kmo_table)\n",
    "\n",
    "# Bartlett’s test\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "chi2, p = calculate_bartlett_sphericity(df)\n",
    "print(f\"Bartlett χ² = {chi2:0.1f}, p = {p:0.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a863e-3b6b-4c88-9bce-a1896d78d19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run PCA on portrayed and viewer emotions (separately) \n",
    "## here for portrayed; switch out csv to run for viewer emotions \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def parse_and_average(cell):\n",
    "    if pd.isna(cell):\n",
    "        return np.nan\n",
    "    s = str(cell).strip()\n",
    "    if not s:\n",
    "        return np.nan\n",
    "    parts = s.split(',')\n",
    "    numeric_values = []\n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        if part:\n",
    "            try:\n",
    "                numeric_values.append(float(part))\n",
    "            except ValueError:\n",
    "                return np.nan\n",
    "    if len(numeric_values) == 0:\n",
    "        return np.nan\n",
    "    return sum(numeric_values) / len(numeric_values)\n",
    "\n",
    "# 1. Read the CSV\n",
    "df = pd.read_csv(\"portrayed_emotions_git.csv\", header=0)\n",
    "\n",
    "# 2. Parse multiple values by averaging\n",
    "df_parsed = df.apply(lambda series: series.map(parse_and_average))\n",
    "\n",
    "# 3. Convert to numeric-only columns, fill missing data\n",
    "df_numeric = df_parsed.select_dtypes(include=[np.number]).copy()\n",
    "df_numeric.fillna(df_numeric.mean(), inplace=True)\n",
    "\n",
    "# 4. Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_numeric)\n",
    "\n",
    "# 5. Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "pca_scores = pca.transform(scaled_data)\n",
    "\n",
    "# 6. Scree plot\n",
    "plt.plot(range(1, pca.n_components_ + 1),\n",
    "         pca.explained_variance_ratio_,\n",
    "         marker='o')\n",
    "plt.title(\"Scree Plot\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Variance Explained\")\n",
    "plt.show()\n",
    "\n",
    "# 7. PCA loadings\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=[f\"PC{i+1}\" for i in range(pca.n_components_)],\n",
    "    index=df_numeric.columns\n",
    ")\n",
    "print(\"PCA Loadings:\")\n",
    "print(loadings)\n",
    "\n",
    "print(\"\\nExplained Variance Ratio:\")\n",
    "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"PC{i+1}: {ratio:.4f}\")\n",
    "\n",
    "# Extract and save the first 4 PCs’ loadings\n",
    "\n",
    "loadings_first_4 = loadings[[f\"PC{i+1}\" for i in range(4)]]\n",
    "loadings_first_4.to_csv(\"portrayed_first_4_pcs_loadings.csv\")\n",
    "print(\"\\nLoadings for the first 4 PCs saved to 'portrayed_first_4_pcs_loadings.csv'.\")\n",
    "\n",
    "# Create a DataFrame of the first 4 PC scores for each observation\n",
    "\n",
    "pca_scores_df = pd.DataFrame(\n",
    "    pca_scores,\n",
    "    columns=[f\"PC{i+1}\" for i in range(pca.n_components_)]\n",
    ")\n",
    "pca_scores_first_4 = pca_scores_df[[f\"PC{i+1}\" for i in range(4)]]\n",
    "\n",
    "# Save the first 4 PC scores\n",
    "pca_scores_first_4.to_csv(\"portrayed_first_4_pcs_scores.csv\", index=False)\n",
    "print(\"Scores for the first 4 PCs saved to 'portrayed_first_4_pcs_scores.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a332e1-21dd-466e-a967-7622693626c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the pairwise correlations of raters rating the same scene, \n",
    "# then correct using Spearman-Brown prophecy formula \n",
    "## this is an example for PC1 in portrayed, but files can be edited for all PCs\n",
    "\n",
    "##frist have to assign the correct labels to the rows in PC1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Load a CSV file with 512 rows\n",
    "df = pd.read_csv(\"portrayed_first_4_pcs_scores.csv\")\n",
    "\n",
    "# Safety check\n",
    "assert len(df) == 512, \"Your CSV must have exactly 512 rows.\"\n",
    "\n",
    "# 2) Assign each row a group index (0..7) based on its row index\n",
    "df['group_index'] = df.index // 64   # rows [0..63] -> group 0, [64..127] -> 1, etc.\n",
    "\n",
    "# 3) Assign each row a couplet index (0..31) within its group\n",
    "df['couplet_index'] = (df.index % 64) // 2\n",
    "\n",
    "\n",
    "# Here I define, for each of the 8 groups (0..7),\n",
    "# which couplet indices belong to AM_high, AM_low, MD_high, MD_low.\n",
    "# this is from the MATLAB code for peaks and troughs\n",
    "\n",
    "\n",
    "groups_couplet_labels = {\n",
    "    0: {  # This is group_index 0 (rows 0..63) -> 32 couplets labeled as you choose\n",
    "        \"AM_high\": [0, 1, 3, 5, 7, 10, 14, 17, 18, 19, 20, 22, 25, 27, 29, 30],\n",
    "        \"AM_low\":  [2, 4, 6, 8, 9, 11, 12, 13, 15, 16, 21, 23, 24, 26, 28, 31],\n",
    "        \"MD_high\": [],\n",
    "        \"MD_low\":  []\n",
    "    },\n",
    "    1: {  # group_index 1 (rows 64..127)\n",
    "        \"AM_high\": [0, 3, 5, 7, 10, 12, 13, 16, 18, 19, 20, 24, 26, 28, 29, 30],\n",
    "        \"AM_low\":  [1, 2, 4, 6, 8, 9, 11, 14, 15, 17, 21, 22, 23, 25, 27, 31],\n",
    "        \"MD_high\": [],\n",
    "        \"MD_low\":  []\n",
    "    },\n",
    "    2: {\n",
    "        \"AM_high\": [1, 2, 3, 7, 9, 13, 16, 17, 19, 20, 22, 23, 25, 28, 29, 30],\n",
    "        \"AM_low\":  [0, 4, 5, 6, 8, 10, 11, 12, 14, 15, 18, 21, 24, 26, 27, 31],\n",
    "        \"MD_high\": [],\n",
    "        \"MD_low\":  []\n",
    "    },\n",
    "    3: {\n",
    "        \"AM_high\": [0, 3, 4, 5, 6, 9, 10, 12, 14, 17, 18, 21, 23, 27, 28, 30],\n",
    "        \"AM_low\":  [1, 2, 7, 8, 11, 13, 15, 16, 19, 20, 22, 24, 25, 26, 29, 31],\n",
    "        \"MD_high\": [],\n",
    "        \"MD_low\":  []\n",
    "    },\n",
    "    4: {\n",
    "        \"AM_high\": [],\n",
    "        \"AM_low\":  [],\n",
    "        \"MD_high\": [2, 4, 5, 6, 8, 9, 11, 13, 15, 16, 18, 20, 21, 22, 24, 29],\n",
    "        \"MD_low\":  [0, 1, 3, 7, 10, 12, 14, 17, 19, 23, 25, 26, 27, 28, 30, 31]\n",
    "    },\n",
    "    5: {\n",
    "        \"AM_high\": [],\n",
    "        \"AM_low\":  [],\n",
    "        \"MD_high\": [1, 2, 4, 5, 6, 9, 10, 11, 14, 20, 21, 23, 24, 25, 26, 29],\n",
    "        \"MD_low\":  [0, 3, 7, 8, 12, 13, 15, 16, 17, 18, 19, 22, 27, 28, 30, 31]\n",
    "    },\n",
    "    6: {\n",
    "        \"AM_high\": [],\n",
    "        \"AM_low\":  [],\n",
    "        \"MD_high\": [0, 3, 4, 8, 10, 11, 13, 14, 15, 20, 21, 23, 26, 27, 28, 29],\n",
    "        \"MD_low\":  [1, 2, 5, 6, 7, 9, 12, 16, 17, 18, 19, 22, 24, 25, 30, 31]\n",
    "    },\n",
    "    7: {\n",
    "        \"AM_high\": [],\n",
    "        \"AM_low\":  [],\n",
    "        \"MD_high\": [2, 4, 7, 8, 9, 10, 14, 15, 17, 18, 20, 21, 23, 24, 28, 30],\n",
    "        \"MD_low\":  [0, 1, 3, 5, 6, 11, 12, 13, 16, 19, 22, 25, 26, 27, 29, 31]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# 4) Build a 'label' column based on group_index and couplet_index\n",
    "#    using the dictionary above.\n",
    "\n",
    "# Initialize an empty column (or you can do it in one step).\n",
    "df['label'] = None\n",
    "\n",
    "for group_idx, label_dict in groups_couplet_labels.items():\n",
    "    # For each label (AM_high, AM_low, MD_high, MD_low), get the list of couplets\n",
    "    for label_name, couplets_list in label_dict.items():\n",
    "        mask = (df['group_index'] == group_idx) & (df['couplet_index'].isin(couplets_list))\n",
    "        df.loc[mask, 'label'] = label_name\n",
    "\n",
    "\n",
    "# 5) Save to a new CSV and print\n",
    "df.to_csv(\"portrayed_PC_scores_labeled_by_movie.csv\", index=False)\n",
    "print(df.head(70))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5211dfe4-b79d-43fb-ae22-659a3cfec852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can compute the pairwise correlations of raters rating the same scene, \n",
    "# then correct using Spearman-Brown prophecy formula \n",
    "## (again: this is an example for PC1 in portrayed, but files can be edited for all PCs)\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def eight_pairwise_correlations(\n",
    "    csv_path: str,\n",
    "    column_name: str,\n",
    "    *,\n",
    "    output_csv_path: str | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute eight Pearson correlations down a single column (512 rows).\n",
    "\n",
    "    For each 64‑row block:\n",
    "      even‑indexed rows (0,2,…,62)  vs.  odd‑indexed rows (1,3,…,63)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_path          Path to the source CSV.\n",
    "    column_name       Header label of the column to analyse.\n",
    "    output_csv_path   If provided, write the 8‑row results table to this file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame with columns: block, pearson_r, p_value\n",
    "    \"\"\"\n",
    "    # ---- read and pull the chosen column ----\n",
    "    col = pd.read_csv(csv_path)[column_name].to_numpy(dtype=float)\n",
    "\n",
    "    if col.size < 512:\n",
    "        raise ValueError(f\"Column '{column_name}' has {col.size} rows; need ≥ 512.\")\n",
    "\n",
    "    # ---- crunch the eight correlations ----\n",
    "    rows_per_block = 64\n",
    "    results = []\n",
    "\n",
    "    for block in range(8):\n",
    "        seg = col[block * rows_per_block : (block + 1) * rows_per_block]\n",
    "\n",
    "        even = seg[::2]   # 32 values\n",
    "        odd  = seg[1::2]  # 32 values\n",
    "\n",
    "        if even.size != 32 or odd.size != 32:\n",
    "            raise ValueError(f\"Block {block+1} doesn’t have 32 values per group.\")\n",
    "\n",
    "        r, p = pearsonr(even, odd)\n",
    "        results.append({\"block\": block + 1, \"pearson_r\": r, \"p_value\": p})\n",
    "\n",
    "    out_df = pd.DataFrame(results)\n",
    "\n",
    "    # save -- to then do SB correction below\n",
    "    if output_csv_path:\n",
    "        out_df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"Results saved to '{output_csv_path}'\")\n",
    "\n",
    "    return out_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tbl = eight_pairwise_correlations(\n",
    "        \"portrayed_PC_scores_labeled_by_movie.csv\",\n",
    "        column_name=\"PC1\",              # <- just the header label\n",
    "        output_csv_path=\"PC1_portrayed_pearson_corrs.csv\" # omit this line if you don’t want a file\n",
    "    )\n",
    "    print(tbl)\n",
    "\n",
    "\n",
    "## then correct with SB \n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV (adjust the path as needed)\n",
    "df = pd.read_csv(\"PC1_portrayed_pearson_corrs.csv\")\n",
    "\n",
    "# Calculate Spearman-Brown (2-rater) corrected reliability.\n",
    "sb_values = (2 * df[\"pearson_r\"]) / (1 + df[\"pearson_r\"])\n",
    "\n",
    "# Insert it right after the 'mean_correlation' column.\n",
    "df.insert(\n",
    "    loc=df.columns.get_loc(\"pearson_r\") + 1,  # position right after\n",
    "    column=\"sb_corrected\",\n",
    "    value=sb_values\n",
    ")\n",
    "\n",
    "# Save back to CSV with the new column\n",
    "df.to_csv(\"portrayed_SB_corrected_PC1_pearson_correlations.csv\", index=False)\n",
    "\n",
    "# view \n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b3d935-b418-4390-85bd-82ea1bd0ab2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
